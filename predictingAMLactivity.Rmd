---
title: "Performance Evaluation of Classifiers"
author: "Dr. Shila Ghazanfar, Dr. Dario Strbenac, Dr. Ellis Patrick and Prof. Jean Yang"
date: "29 November 2018"
output:
  html_document:
    code_folding: show
    number_sections: yes
    self_contained: yes
    toc: true
    toc_float: true
    toc_depth: 1
    theme: yeti
---

# Activity Overview

- Acute Myeloid Leukaemia Data Set

- Feature-based Classification

- Network-based Classification

- Evaluation of Feature Selection

- Evaluation of Prediction Performance Metrics

```{r, echo = FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(results = "show", fig.show = "show")
options(width = 91)
```

# Acute Myeloid Leukaemia Treatment Resistance Data Set

- Primary therapy resistance is a major problem in acute myeloid leukaemia (AML) treatment. Approximately 20-30% of younger adult patients with AML and as many as 50% of older adults are refractory to induction treatment.

- Research findings are <a href="http://www.haematologica.org/content/103/3/456" target="_blank">published</a> in *Haematologica* in 2018.

- The data is available from <a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE106291" target="_blank">GEO Browser</a> as 250 `txt.gz` files of gene-level read counts or a Microsoft Excel file where the gene expression values were standardised to have a mean of 0 and variance of 1.

- For details on exactly how to download and process this data from GEO, see our [June local workshop notes](https://github.com/SydneyBioX/localWorkshop), in particular Activity 1. We have provided an RData file containing the objects needed for the remainder of this workshop.

There are two data objects; `measurementsVS` has variance-stabilised gene expression data and `sampleInfo` contains the corresponding clinical data, including a patient's response status.

```{r}
load(url("https://github.com/SydneyBioX/TrainClassifyR/blob/master/data/AMLgeneClinical.RData?raw=true"))
classes <- sampleInfo[, "Response"]
```

The measurements are numeric and the clincal data is a combination of numeric and categorical variables.

```{r}
measurementsVS[1:5, 1:5]
head(sampleInfo)
```

```{r}
table(classes)
```

The data set is class-imbalanced. There are approximately twice as many sensitive patients as there are resistant patients.

To begin using ClassifyR, firstly load it.

```{r, message = FALSE}
library(ClassifyR)
```

`plotFeatureClasses` is a function that may be used to plot the density and/or a dot plot of numeric variables or a bar chart of categorical variables. Perform a simple quality check with the clinical data. ZFY is a gene on the Y chromosome which only males have. Plot its expression with the gender in place of the treatment resistance classes.

```{r, fig.height = 4, fig.width = 7, fig.align = "center"}
plotFeatureClasses(measurementsVS, sampleInfo[, "Gender"], targets = "ZFY",
                   whichNumericFeaturePlots = "density", xAxisLabel = "RNA-seq Abundance")
```

The abundance of ZFY grouped by gender is as expected.

# Feature-based Classification

In this section, two classifiers will be built and compared. One classifies based on a change of means and the other classifiers using the overall distribution of measurements of each feature, followed by voting.

## DLDA Classification: Changes in Features' Means

- The default feature selection method of `SelectParams` is an ordinary t-test based ranking and selection of the top $p$ genes that give the best resubstitution error (considering 10, 20, ..., 100 top-ranked features). See `?SelectParams` for the specification.

- The default training and prediction methods for `TrainParams` and `PredictParams` are for Diagonal Linear Discriminant Analysis (DLDA). See `?TrainParams` and `?PredictParams` for the specification.

A 10 permutation and 5 folds cross-validation using default selection and classification methods is done using `runTests`.

```{r}
classifiedDM <- runTests(measurements = measurementsVS, 
                         classes = classes, 
                         datasetName = "AML",
                         classificationName = "Changes in Means",
                         permutations = 1, # For testing tutorial.
                         seed = 2018,
                         parallelParams = SerialParam())
classifiedDM
```

- Access the chosen features by using the `features` accessor, which extracts all of the feature selections at each iteration of cross-validation. View the features chosen in folds 1 and 2 of permutation 1.
<br>
The features are in a list of lists. The top-level list contains one element for each iteration and the second-level list contains one element for each fold.

```{r}
# Permutation 1, folds 1 and 2.
features(classifiedDM)[[1]][1:2]
```

- The `predictions` accessor gets all of the class predictions. View the first few predictions of the first permutation
<br>
The predictions are stored in a list, one for each permutation and have as many rows as there are samples. Use `head` to limit the number of predictions displayed.

```{r}
# Permutation 1
head(predictions(classifiedDM)[[1]])
```

The `distribution` function calculates the feature selection frequency of all features. Use `?distribution` to find out about it and determine the most frequently selected feature. Use the `sort` on the output of the `distribution` function.

### Most Frequently Selected Feature

```{r}
frequencies <- distribution(classifiedDM, plot = FALSE, summary = "count")
frequencies <- sort(frequencies, decreasing = TRUE)
head(frequencies)
```

`r names(frequencies)[1]` is chosen `r frequencies[1]` out of 50 possible times.

The distribution of gene expression per class can be quickly visualised with `plotFeatureClasses`. The `targets` parameter of `plotFeatureClasses` specifies one or more features to be plotted.

```{r, fig.height = 4, fig.width = 7, fig.align = "center"}
plotFeatureClasses(measurementsVS, classes, 
                   targets = names(frequencies)[1],
                   whichNumericFeaturePlots = "density",
                   xAxisLabel = "RNA-seq Abundance")
```

The gene is visibly differentially expressed between resistant and sensitive patients.

## naive Bayes Classification: Changes in Features' Distributions

Create a selection parameter object specifying the use of Kullback-Leibler feature selection. `KullbackLeiblerSelection` is the name of the function to be specified.

```{r}
selectParams <- SelectParams(KullbackLeiblerSelection,
                             resubstituteParams = ResubstituteParams())
```

By default, the mean is the location and the standard deviation is the scale.

A variety of DD classifiers are available in ClassifyR. For this example, the naive Bayes method will be used. The difference of the height (scaled by the number of samples in each class) between the kernel densities of the two classes is used by each gene to vote for one class. The class with the most votes is the predicted class of the patient.

Create a training parameter object specifying the use of the naive Bayes classifier.

```{r}
trainParams <- TrainParams(naiveBayesKernel)
```

Create a prediction parameter object specifying the use the height difference between densities and unweighted voting. `naiveBayesKernel` trains a classifier and returns a factor vector of class predictions, so there is no other function used for predictions.
<br>
`?naiveBayesKernel` will show the names of the parameters controlling the voting process.

```{r}
predictParams <- PredictParams(NULL, weighted = "unweighted", weight = "height difference")
```

As was done for DM classification, 2 permutation and 5 fold cross-validation is done. It takes longer to complete than DM classification, because thousands of kernel densities are being esimated for each iteration. If working on a machine with many processors, increasing the number of permutations is desirable.

```{r}
classifiedDD <- runTests(measurements = measurementsVS, 
                         classes = classes, 
                         datasetName = "AML", 
                         classificationName = "Changes in Distributions",
                         params = list(selectParams, trainParams, predictParams),
                         permutations = 1,
                         seed = 2018,
                         parallelParams = SerialParam())
classifiedDD
```

Evaluation of these classifiers will be made later.

# Network-based Classification

Rather than treating each gene individually in a trained model, publicly available databases can be used to create metafeatures or pairwise relationships to be used for classification.

## MSigDB Hallmarks for a Gene Set Classifier

Perhaps the simplest network-inspired classification is to take a collection of gene sets, summarise them by the median abundance value of their genes, and do classification with the reduced data set.

MSigDB's GMT files have a variable number of elements per line. It is easiest to import them with the `read.gmt` function provided by another R package available from Bioconductor named qusage. FeatureSetCollection is a container for storing a collection of sets and showing a nice summary of the collection.

```{r}
library(qusage)
hallmarkList <- read.gmt("https://raw.githubusercontent.com/SydneyBioX/TrainClassifyR/master/databases/hallmarks.gmt")
names(hallmarkList) <- gsub("HALLMARK_", '', names(hallmarkList))
hallmarkSet <- FeatureSetCollection(hallmarkList)
hallmarkSet
```

Next, summarise the gene abundances into a single value per gene set.

```{r}
geneSetMatrix <- featureSetSummary(measurementsVS, featureSets = hallmarkSet)
dim(geneSetMatrix)
```

Input this feature-reduced matrix to any classification algorithm. For example, use t-test selection and DLDA classification. `nFeatures` must be changed from its default of considering the top 10, 20, ..., 100 ranked features because there are now only 50 features.

```{r}
resubstituteParams <- ResubstituteParams(nFeatures = 1:10,
                                         performanceType = "balanced error",
                                         better = "lower")
selectParams <- SelectParams(tTestSelection,
                             resubstituteParams = resubstituteParams)
 
classifySets <- runTests(geneSetMatrix, classes, datasetName = "AML",
                         classificationName = "Hallmarks Response",
                         permutations = 1,
                         params = list(selectParams, TrainParams(),
                                       PredictParams())
                         )
```

## BioPlex 2.0 Binary Interactions for a Network-based Classifier <span style="color:red;font-weight:bold;">(Advanced, Optional)</span>

BioPlex is a large database of protein-protein interactions, [published in 2017](https://www.nature.com/articles/nature22366). If a particular gene has a larger abundance value than another gene in a particular sample could be an informative relationship for making predictions with. However, with about 20000 genes measured per experiment, exhaustively doing feature selection for all possible pairs is impractical. BioPlex can be used to reduce the complexity of feature selection by testing only the pairs of genes with experimental evidence of directly interacting with each other.

This is conceptually similar to the [k-Top Scoring Pairs (k-TSP) classifier](https://academic.oup.com/bioinformatics/article/31/2/273/2365798
), but the feature selection is biologically guided.

```{r}
BioPlexFileURL <- "https://github.com/SydneyBioX/TrainClassifyR/blob/master/databases/BioPlexInteractions.tsv?raw=true"
proteinPairs <- read.table(BioPlexFileURL, sep = '\t', stringsAsFactors = FALSE,
                           header = TRUE, check.names = FALSE)
head(proteinPairs)
```

Create a Pairs object of the interactions.

```{r}
proteinPairs <- Pairs(proteinPairs[, "SymbolA"], proteinPairs[, "SymbolB"])
proteinPairs
```

Do feature selection using `pairsDifferencesSelection` and classification using `kTSPclassifier`. Renaming of a variable created internally by feature selection during the cross-validation process in `runTest` is necessary before it can be used by `kTSPclassifier`.

```{r, eval = FALSE}
selectParams <- SelectParams(pairsDifferencesSelection, resubstituteParams = resubstituteParams,                             featurePairs = proteinPairs, subsetToSelections = FALSE)
trainParams <- TrainParams(kTSPclassifier, weighted = "unweighted",                         
                           intermediate = setNames("selectedFeatures", "featurePairs"))
predictParams <- PredictParams(NULL)
results <- runTests(measurementsVS, classes, dataset = "AML",                   
                    classificationName = "BioPlex kTSP",
                    permutations = 1,
                    params = list(selectParams, trainParams, predictParams),
                    parallelParams = SerialParam())
```

# Evaluation of Feature Selection

Two aspects of feature selection may be evalutated; stability and commonality. Stability evaluates how similar the selected features are within a cross-validation scheme whereas commonality makes comparisons between different feature selection methods.

## Feature Selection Stability

- If the genes being selected are the same ones in most of the cross-validations, then the classifier has good stability.

- `selectionPlot` provides a way to compare all pairs of gene selections within a classifier or between classifiers.

- Input is a list of `ClassifyResult` objects.

-  For 20 permutations and 5 folds, there are $^{100}C_2 = 4950$ overlaps to compute. Like `runTests`, `selectionPlot` may utilise multiple processors.

Plot the distribution of overlaps of selected features of the DM and DD classifiers.

```{r, fig.align = "center", fig.width = 10, fig.height = 3}
withinChoices <- selectionPlot(list(classifiedDM, classifiedDD),
                               xVariable = "selectionName", 
                               xLabel = "Selection Method",
                               columnVariable = "None",
                               boxFillColouring = "None", 
                               boxLineColouring = "None",
                               rotate90 = TRUE,
                               parallelParams = SerialParam())
```

KL divergence selection is more stable to different subsets of samples than the ordinary t-test.

## Changing Elements of Saved Plots

- Almost all plots produced by ClassifyR are `ggplot` objects created by ggplot2.

- Such objects can be customised after creation because ggplot2 uses a 'painting-over' graphics model, unlike base R graphics.

Change the plot title to "Chosen Genes Overlaps". Use the `ggtitle` function from ggplot2.

```{r, fig.align = "center", fig.width = 10, fig.height = 3}
library(ggplot2)
withinChoices + ggtitle("Chosen Genes Overlaps")
```

## Feature Selection Commonality

- The features being selected by different classifications can be compared.

- Again, `selectionPlot` is used.

Compare the overlaps in features selected between DM and DD classifiers. The `comparison` parameter of `selectionPlot` controls what kind of comparison is made.

```{r, fig.align = "center", fig.width = 10, fig.height = 3}
betweenChoices <- selectionPlot(list(classifiedDM, classifiedDD),
                                comparison = "selectionName",
                                xVariable = "selectionName", xLabel = "Selection Method",
                                columnVariable = "None",
                                boxFillColouring = "None", boxLineColouring = "None",
                                rotate90 = TRUE)
```

Note that the sets of features chosen by the DM and DD classifiers have little in common.

# Evaluation of Prediction Performance Metrics

- `calcCVperformance` calculates performance metrics for `ClassifyResult` objects. 12 different metrics can be calculated. You can calculate these performance metrics even on external sets of actual and predicted classes with `calcExternalPerformance`.

- Metrics are all applicable to data sets with two *or more* classes (except Matthews Correlation Coefficient).

- `calcExternalPerformance` can be used on a pair of factor vectors of the same length. For example,

```{r}
actualClasses <- factor(c("Yes", "Yes", "No", "No", "No"))
predictedClasses <- factor(c("Yes", "No", "No", "No", "No"))
calcExternalPerformance(actualClasses, predictedClasses, "error")
calcExternalPerformance(actualClasses, predictedClasses, "accuracy")
```

## Balanced Error Rate

- Class sizes of resistance data set are imbalanced. Errors should be summarised by the balanced error rate.

For each of the three classifications done earlier, calculate the balanced error rate using `calcCVperformance`. The value of the parameter named `performanceType` needs to be changed from its default, which is the ordinary error rate.

- Note that the Performance Measures field of the ClassifyResult object summary is no longer empty after calculating balanced error rate.

```{r}
classifiedDM <- calcCVperformance(classifiedDM, "balanced error")
classifiedDD <- calcCVperformance(classifiedDD, "balanced error")

classifiedDM # Note that Performance Measures is no longer empty.
performance(classifiedDM)
```

## Distribution of Balanced Error Rate

`performancePlot` can be used to plot the distribution of a metric to see its stability. The set of samples was predicted 20 times by each classifier.

Compare the distributions of balanced error rates of the three classifiers. The value of the `performanceName` parameter needs to be changed to specify the balanced error rate.

```{r, fig.width = 14, fig.height = 4}
errorPlot <- performancePlot(list(classifiedDM, classifiedDD),
                             performanceName = "Balanced Error Rate",
                             boxFillColouring = "None", boxLineColouring = "None",
                             columnVariable = "None", title = "Balanced Errors",
                             xLabel = "Classifier", rotate90 = TRUE, plot = FALSE)
errorPlot + geom_hline(yintercept = 0.5, colour = "red")
```

Note that DM classification is the only classifier which does substantially better than random, with an error rate below 50%.

## Precision, Recall, F1 Score

- Micro and macro versions of these metrics can be similarly calculated to the error rates demonstrated previously.

- Use the macro version because each class makes an equal contribution to the metric, unlike for the micro version, which can give the false impression of good performance for a class-imbalanced data set.

Calculate the macro precision for the DM classifier using `calcCVperformance`.

```{r}
classifiedDM <- calcCVperformance(classifiedDM, "macro precision")
names(performance(classifiedDM))
performance(classifiedDM)[["Macro Precision"]]
```

## Sample-specific Error Rate

Calculate the sample-specific error rates for each patient. Use again the function named `calcCVperformance`.

```{r}
classifiedDM <- calcCVperformance(classifiedDM, "sample error")
classifiedDD <- calcCVperformance(classifiedDD, "sample error")
```

Plot a heatmap of sample-wise errors using `samplesMetricMap`. Changing the value of `showXtickLabels` to `FALSE` removes the sample labels from the x-axis, which would otherwise look cluttered.

```{r, fig.width = 10, fig.height = 5}
errorPlot <- samplesMetricMap(list(classifiedDM, classifiedDD),
                              xAxisLabel = "Samples", yAxisLabel = "Classifier",
                              showXtickLabels = FALSE)
```

DLDA accounts for the class imbalance of the data set and performs moderately well but the naive Bayes classifier is adversely affected by it. It is evident that the naive Bayes classifier is simply classifying all samples as being the majority class (Sensitive).